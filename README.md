## 机器学习的一般流程

### 数据收集

### ETL
确定数据收集范围

### 数据读取

### 数据格式转换

### EDA

### 数据清洗
#### 缺失值处理

* 丢弃
* 平均值
* 中位数
* 预测值
* One-Hot

#### 样本均衡
#### 异常值处理

* 高斯
* OneCLassSVM
* KNN

### 特征工程

#### 特征选择

##### Filter

* 方差选择法:VarianceThreshold
* 相关系数法:SelectKBest
* 通过卡方检验:SelectKBest
* 互信息法:SelectKBest

##### Wrapper
使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

* 递归消除特征法:RFE

##### Embedded
使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

* 基于惩罚项的特征选择法:SelectFromModel  
若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，需要构建一个新的逻辑回归模型。

* 基于树模型的特征选择法:SelectFromModel

##### Other

* 构建深度学习模型选择某一层作为特征
* 稳定性选择:在不同特征子集上运行特征选择算法统计某个特征被认为是重要特征的频率
* 引入第三方数据构造新特征
* 组合特征

#### 离散特征

* 聚类
* One-Hot

#### 连续特征

* 忽略掉一些小数位上的变化
* 二值化
* 分箱
* 取值的范围跨度太大或太小，采用非线性变换log/平方/开方
* 多项式数据转换提高非线性能力
* 标准化
* 归一化

#### 时间类型

* 分段：早，中，晚
* 特殊时刻
* 时间间隔

#### 空间特征

#### 文本特征

#### 降维

* SVD
* PCA
* LDA

### 模型选择
#### LR
##### 优点
##### 缺点
##### 数据要求
##### 场景

#### FTRL
#### FTML
#### FFM
#### GBDT
##### 优点
##### 缺点
##### 数据要求
##### 场景

#### DNN
#### Wide&Deep

### 求参策略(LOSS)
#### MSE
#### MLE
#### 交叉熵
#### 平方损失

### 求参算法(优化算法)
#### GD
#### SGD
#### Monentum
#### Adagrad
#### Adam

### 模型调试

#### 错误分析
首先要根据Learning Curve来判断模型处于哪种拟合情况，判断模型结构是否正确。调整时，先进行模型结构调整(feature数量，多项式元素)，让模型处于Just Right的情况，再调整非结构参数(正则化参数)。

#### 训练集调整

* 增加训练集: Fix High Variance

#### 参数调整
调整时，先进行模型结构调整(feature数量，多项式元素)，让模型处于Just Right的情况，再调整非结构参数(正则化参数)。

* 减小正则化参数的λ值: Fix High Bias
* 增大正则化参数的λ值: Fix High Variance

##### 特征调整

* 减少特征维度（从已有的特征中挑选出一部分）: Fix High Variance
* 增加新特征: Fix High Bias
* 增加多项式元素（比如将特征平方后叠加到原特征上，相当于增加了非线性的输入）: Fix High Bias

### 模型集成
#### Bagging
#### Boosting
#### Stacking

### 生成测试集

### 模型评估

### 数据更新

### 模型更新

## 一些例子
