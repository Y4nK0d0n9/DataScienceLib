## 机器学习的一般流程

### 数据收集

### ETL
确定数据收集范围

### 数据读取

### 数据格式转换

### EDA

### 数据清洗
#### 去噪
去除作弊数据

#### 缺失值处理

* 丢弃
* 平均值
* 中位数
* 预测值
* One-Hot

#### 样本均衡
Paper：Learning from Imbalanced Data

* 上采样：利用对稀有类样本的分布模拟生成临近的一些样本。
* 下采样：聚类，每一类按照比例抽取部分样本。
* 使用Boosting对稀有类样本赋予较高权重。

#### 异常值处理

* 高斯
* OneCLassSVM
* KNN

### 特征工程

#### 特征选择

##### Filter

* 方差选择法:VarianceThreshold
* 相关系数法:SelectKBest
* 通过卡方检验:SelectKBest
* 互信息法:SelectKBest
* 去除共线性变量
* 对于连续型的自变量和二元的离散变量，利用WOE，IV，通过WOE的变化来调整出最佳的分箱阀值，通过IV值，筛选出有较高预测价值的自变量。

##### Wrapper
使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

* 递归消除特征法:RFE

##### Embedded
使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

* 基于惩罚项的特征选择法:SelectFromModel  
若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，需要构建一个新的逻辑回归模型。

* 基于树模型的特征选择法:SelectFromModel

##### Other

* 构建深度学习模型选择某一层作为特征
* 稳定性选择:在不同特征子集上运行特征选择算法统计某个特征被认为是重要特征的频率
* 引入第三方数据构造新特征
* 组合特征
显式特征组合
1. 需要进行特征离散化；针对不同的特征类型，有不同的处理方式。
numerical feature: 根据统计量进行等频、等宽、分位点等划分区间，1R 方法，Entropy-Based Discretization。
ordinal feature: 有序特征离散化可编码为 (1,0,0),(1,1,0),(1,1,1)。
categorical feature：无序特征离散化用dummy-encoding将一维信息嵌入模型的 bias 中，起到简化逻辑回归模型的作用，降低了模型过拟合的风险。one-hot每个分类型变量的各个值在模型中都可看作独立变量，增强拟合能力。当模型加正则化的情况下约束模型自由度，效果更好。 feature-hash 将高维稀疏特征映射到固定维度空间。
半显式特征组合
基于树方法形成特征划分并给出相应组合路径，类似GBDT-LR。
* 将连续值特征进行离散化后喂入 gbdt，效果不佳，AUC 比较低。这是因为 gbdt 本身能很好的处理非线性特征，使用离散化后的特征反而没什么效果。xgboost 等树模型无法有效处理高维稀疏特征比如 user id 类特征，可以采用的替代方式是: 将这类 id 利用一种方式转换为一个或多个新的连续型特征，然后用于模型训练。
* 当采用叶子结点的 index 作为特征输出需要考虑每棵树的叶子结点并不完全同处于相同深度。
* 采用 Monte Carlo Search 对 xgboost 的众多参数进行超参数选择。
* 在离线训练阶段采用基于 Spark 集群的 xgboost 分布式训练，而在线预测时则对模型文件直接进行解析。单纯采用 xgboost 自动学到的高阶组合特征后续输入 LR 模型并不能完全替代人工特征工程的作用；可以将原始特征以及一些人工组合的高阶交叉特征同 xgboost 学习到的特征组合一起放入后续的模型。
2. 然后进行笛卡尔积、内积。

#### 离散特征

* 聚类
* One-Hot

#### 连续特征

* 忽略掉一些小数位上的变化
* 二值化
* 分箱
* 取值的范围跨度太大或太小，采用非线性变换log/平方/开方
* 多项式数据转换提高非线性能力
* 标准化
* 归一化

#### 时间类型

* 分段：早，中，晚
* 特殊时刻
* 时间间隔

#### 空间特征

#### 文本特征

#### 降维

* SVD
* PCA
* LDA

### 模型选择

#### LR
##### 优点
##### 缺点
##### 数据要求
##### 场景

#### FTRL

#### FTML

#### FFM

#### GBDT
##### 优点
##### 缺点
##### 数据要求
##### 场景

#### DNN
#### Wide&Deep

### 求参策略(LOSS)
#### MSE
#### MLE
#### 交叉熵
#### 平方损失

### 求参算法(优化算法)
#### GD
#### SGD
#### Monentum
#### Adagrad
#### RMSProp
#### AdaDelta
#### Adam

### 模型调试
#### BaseLine

* 随机生成结果
* 简单模型默认参数同时可作特征选择
* 选择模型使用默认参数对比结构

#### 错误分析
首先要根据Learning Curve来判断模型处于哪种拟合情况，判断模型结构是否正确。调整时，先进行模型结构调整(feature数量，多项式元素)，让模型处于Just Right的情况，再调整非结构参数(正则化参数)。

#### 训练集调整

* 增加训练集: Fix High Variance

#### 参数调整
gridsearch/randomsearch/hyperopt
调整时，先进行模型结构调整(feature数量，多项式元素)，让模型处于Just Right的情况，再调整非结构参数(正则化参数)。

* 减小正则化参数的λ值: Fix High Bias
* 增大正则化参数的λ值: Fix High Variance

##### 特征调整

* 减少特征维度（从已有的特征中挑选出一部分）: Fix High Variance
* 增加新特征: Fix High Bias
* 增加多项式元素（比如将特征平方后叠加到原特征上，相当于增加了非线性的输入）: Fix High Bias

### 模型集成
#### Bagging
#### Boosting
#### Stacking

### 生成测试集

### 模型评估

### 数据更新

### 模型更新

## 一些例子
