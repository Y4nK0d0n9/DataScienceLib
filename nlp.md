## 语言模型是什么
通常用条件概率表示，对有意义的句子概率大，对没意义的句子概率小。

### N-gram neural model
用每个n-gram的历史n-1个词语组成的内容来预测第n个词。

### Continuous Bag-of-Words model(CBOW)
不考虑上下文的词语输入顺序，用上下文词语的词向量的均值来预测当前词。好处是对上下文词语的分布在词向量上进行了平滑，去掉了噪声，因此在小数据集上很有效。

### Skip-gram model
将一个词的词向量映射到2n个词的词向量（2n表示当前输入词的前后各n个词），然后分别通过softmax得到这2n个词的分类损失值之和。