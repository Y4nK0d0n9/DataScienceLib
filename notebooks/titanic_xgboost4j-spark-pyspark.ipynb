{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile(\"/Users/yunky/spark-2.3.3-bin-hadoop2.7/python/lib/sparkxgb-0.72.zip\")\n",
    "from sparkxgb import XGBoostEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "  [StructField(\"PassengerId\", DoubleType()),\n",
    "    StructField(\"Survival\", DoubleType()),\n",
    "    StructField(\"Pclass\", DoubleType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Sex\", StringType()),\n",
    "    StructField(\"Age\", DoubleType()),\n",
    "    StructField(\"SibSp\", DoubleType()),\n",
    "    StructField(\"Parch\", DoubleType()),\n",
    "    StructField(\"Ticket\", StringType()),\n",
    "    StructField(\"Fare\", DoubleType()),\n",
    "    StructField(\"Cabin\", StringType()),\n",
    "    StructField(\"Embarked\", StringType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark\\\n",
    "  .read\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .schema(schema)\\\n",
    "  .csv(\"/Users/yunky/DataSet/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survival|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|        1.0|     0.0|   3.0|Braund, Mr. Owen ...|  male|22.0|  1.0|  0.0|A/5 21171|   7.25| null|       S|\n",
      "|        2.0|     1.0|   1.0|Cumings, Mrs. Joh...|female|38.0|  1.0|  0.0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Sex\")\\\n",
    "  .setOutputCol(\"SexIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")\n",
    "    \n",
    "cabinIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Cabin\")\\\n",
    "  .setOutputCol(\"CabinIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")\n",
    "    \n",
    "embarkedIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Embarked\")\\\n",
    "  .setOutputCol(\"EmbarkedIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"Pclass\", \"SexIndex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"CabinIndex\", \"EmbarkedIndex\"])\\\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBoostEstimator(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Survival\", \n",
    "    predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|PassengerId|prediction|\n",
      "+-----------+----------+\n",
      "|        1.0|       0.0|\n",
      "|        4.0|       1.0|\n",
      "|       14.0|       0.0|\n",
      "|       15.0|       1.0|\n",
      "|       20.0|       1.0|\n",
      "|       28.0|       1.0|\n",
      "|       34.0|       0.0|\n",
      "|       38.0|       0.0|\n",
      "|       50.0|       1.0|\n",
      "|       52.0|       0.0|\n",
      "|       59.0|       1.0|\n",
      "|       60.0|       0.0|\n",
      "|       82.0|       0.0|\n",
      "|       94.0|       0.0|\n",
      "|       96.0|       0.0|\n",
      "|       99.0|       1.0|\n",
      "|      104.0|       0.0|\n",
      "|      105.0|       0.0|\n",
      "|      107.0|       1.0|\n",
      "|      116.0|       0.0|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(testDF).select(\"PassengerId\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(databaseName='default'),\n",
       " Row(databaseName='ml_100'),\n",
       " Row(databaseName='sparktest')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show databases\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'192.168.0.101'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.driver.host\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
